---
title: "Propaganda Wars"
author: "Amanda Franklin-Ryan"
date: "2023-04-25"
output:
  html_document:
    code_folding: hide
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

### Atrocity Propaganda

*"History stopped in 1936- after that, there was only propaganda"* - **George Orwell**

*"We are children no more"*, 15-year-old 'Nayirah'" tells the Congressional Caucus, blinking back tears, her voice cracking as she describes Iraqi soldiers storming the hospital where she volunteered, stealing incubators and throwing babies onto the floor to die.[@guyjohn592010] It's 1991, Vietnam's legacy still looms large and the American public have no desire to be embroiled in yet another distant war, but Nayirah's testimony sways them with the US invading Iraq 3-months later. Except it wasn't true, any of it. Nayirah al-Sabah, the daughter of the Kuwaiti ambassador had never set foot in the hospital, instead she was coached to perfection for her Oscar winning performance by PR firm Hill and Knowlton who were directly employed by the Kuwaiti Government.[@abuhamad1994] [@howfals]

Atrocity propaganda is nothing new, with lurid tales of fabricated massacres stretching back to the Crusades, but now storytelling is easier than ever before; all you need is a mobile and internet connection. There are now more hours of Syrian conflict footage online, than time elapsed since the war started.Almost five million people are currently trapped in Khartoum as Sudanese generals Abdel Fattah al-Burhan and Mohamed Hamdan Daglo vie for power and Twitter is awash with fake footage taken from conflicts in Yemen, Libya and even a computer game.[@tanksan] With most Twitter accounts geolocation data switched off, it's game on for opportunists worldwide to create and share their own realities. Even with image metadata and reverse image search, it's almost impossible to ascertain whether shared images are accurate, misrepresentative, staged or concocted by government backed bots. [DALL.E2](https://openai.com/product/dall-e-2) will only make things worse, with the potential power to conjure up bespoke atrocities in every style from gritty World War II grayscale to heartrending full colour closeup at the click of a mouse.

Yet in a world where fiction seems higher definition than reality, satellite imagery could offer a glimmer of hope. In March 2022 Ukrainians reported the rape, murder and torture of hundreds of civilians in Bucha, with bodies strewn across the streets. Unsurprisingly, the Russian Defence Ministry immediately dismissed the allegations as, "*another production of the Kyiv regime for the Western media*", blaming Ukrainian soldiers themselves for the killings. [@mirovalev]However Maxar satellite data corroborated the Ukrainian narrative illustrating the bodies had been present long before Ukrainian forces entered the area.[@buchaki2022]

While [high resolution Maxar data](https://www.maxar.com/products/satellite-imagery) is only available commercially or for selected humanitarian disasters, medium resolution Sentinel-1 and Sentinel 2 satellite imagery is freely accessible online. Evidently it's no magic bullet. evidence of newly dug mass graves or bombed buildings, proves only the incident is not a recycled copy-paste of a past atrocity and in isolation can rarely identify the true perpetrator, however it is already being used in conflict mapping. Depending on the location and nature of the violence, burning oil wells; abandoning agricultural land, (pictures here) construction of refugee camps and building damage have been used to identify and monitor armed conflict. Using methods to map meteor craters on the moon, a 2020 study used satellite imagery combined with machine learning to identify unexploded munitions from the Vietnam War embedded in overgrown bomb craters in rural Cambodia.[@lin2020] Detecting chemical and biological warfare with satellite imagery is less promising due to the low concentrations of the agents in the atmosphere, however vegetation changes could potentially be use as a proxy for contamination with radioactivity [@sutlieff2021] and journalists are already using satellite pictures to investigate North Korea's chemical weapon capabilities.[@abirds]

### Raqqa: The Perfect Perpetratorless Crime

Created in 2019, Amnesty International's [Strike Tracker](https://decoders.amnesty.org/projects/strike-tracker#decode-results) used satellite imagery coupled with crowd sourced image labelling to create an accurate timeline for the 2017 US led Coalition bombing in the Syrian city of Raqqa. Attempting to destroy Islamic State's "Caliphate", Coalition forces fired over 30,000 missiles in a four-month bombardment, destroying 80% of the city and killing and injuring thousands of civilians. Pinning down responsibility was challenging as the Coalition accused ISIS, Russia blamed the US and the US denied all responsibility ruling less than one percent of civilian casualty reports "credible".[@april252019]

Amnesty International enlisted thousands of digital activists to analyse a series of satellite photos of the same building to identify the exact date it was destroyed. After following a short interactive tutorial volunteers were presented with a new series of images to label. Combining the crowd sourced strike timeline with witness testimony and interviews, Amnesty forced the Coalition to admit responsibility for 159 civilian deaths.[@isconfl2019]

This project aims to collect the dataset necessary to replicate Amnesty's Strike Tracker Project. However instead of using high resolution human labelled satellite images, medium resolution Sentinel-1 and Sentinel-2 images will be downloaded and in a future seminar paper (hopefully) used to investigate the efficacy of using machine learning methods to automatically detect building damage and to generate a destruction timeline using a simplified version of the methodology described in x. More specifically the project will:

-   Collect and visualise data from UNOSAT illustrating the position of the coalition strikes (for the purposes of the future machine learning model this represents the ground truth)

-   Identify and download suitable Sentinel-2 satellite images from the SentinelHub API, an R function will also be written to simplify this process

-   Create a GIF animation with the downloaded satellite data illustrating the destruction of Raqqa

## Position of Coalition Strikes: UNOSAT Data

Since 2001 the [United Nations Satellite Centre](https://unosat.org/products/) (UNOSAT) has published and analysed geospatial data on climate change, natural disaster and conflict including this [2017 dataset](https://unosat.org/products/1156) on the buildings destroyed by the US-Coalition bombing of Raqqa The shape file contains data taken from five different sets of satellite images from (2013-2015) and includes information on the level of damage (moderate, severe, destroyed); position of the damaged building (latitude, longitude, neighbourhood) and the confidence level in the degree of destruction (Very high, medium, uncertain). It also contains a SiteID column with further information on the nature of the destroyed building (church, university, stadium etc) However the vast majority of buildings are simply labelled "Building (General / Default)". Building destruction has been manually labelled using imagery from the [Pleiades](https://apollomapping.com/pleiades-1-satellite-imagery?gad=1&gclid=CjwKCAjwuqiiBhBtEiwATgvixGBzuGo5Zg4tzpBz0dGRlgkrIHT0QdAfhkbCYzheyUy2ULL6WlvC9xoCR6gQAvD_BwE), [Worldview-2](https://earth.esa.int/eogateway/missions/worldview-2), [Worldview-1](https://earth.esa.int/eogateway/missions/worldview-1) and [Worldview-3](https://apollomapping.com/worldview-3-satellite-imagery?gad=1&gclid=CjwKCAjwuqiiBhBtEiwATgvixIVcMh2NcN7SDkaU5rc7uxCm54hdM_d4nVg09XKar6SwNh2Mtx4fjxoCQVUQAvD_BwE) satellites and has not been verified on the ground.

```{r, messages=FALSE, warnings=FALSE}
library(rio)
library(data.table)
summary <- rio::import("../Visualisations/head_original_raqqa_data.rds")
summary_table <- data.table(summary)
summary_table
```

## Medium Resolution Satellite Data (Sentinel-1 and Sentinel-2)

### Overview

Launched by the European Space Agency in June 2015, primarily for agricultural monitoring and emergency management, Sentinel-2 collects land and ocean imagery to a maximum resolution of 10m every 5 days. [@sentinel]The imagery consists of 13 spectral bands ranging from visible (red, green, blue) to near infrared (NIR) and short wave infrared (SWIR), it also includes specific frequencies for identifying vegetation (bands 5-8a) and clouds (Band 10: Cirrus).[@customs]

Sentinel -1 was launched by the European Commission's Copernicus Project a year earlier in April 2014, and collects Synthetic Aperture Radar (SAR) data every 10 days. [@sentinela]While Sentinel 2's optical sensors allow it to capture imagery from reflected sunlight, Sentinel-1 produces its own electromagnetic beam (with 8 bands between 0.3GHz and 40GHz) and records its reflection. The longer wavelength radar frequencies not only pass straight through clouds so can be used in any weather, but penetrate soil and sand [revealing hidden underground artefacts](https://earthobservatory.nasa.gov/features/SpaceArchaeology). Additionally, phase data can be extracted from the radar signal and used to calculate differences in surface topography with an accuracy of up to a few centimetres. [@earthsciencedatasystems2020] Combining these height difference calculations with machine learning techniques, one 2022 study identified buildings damaged by earthquakes.[@earthsciencedatasystems2020]

Real colour images can be constructed by superimposing red, green and blue bands but often it is more revealing to use false colour. The optimal frequency depends on the exact nature of the task. Infrared is generally preferable for forestry mapping as soil and vegetation are difficult to differentiate in true colour imagery (as their spectral reflectances are almost identical in the visible range), however there is a difference of almost 30% in the mid-infrared band. [@customs]

Frequencies can also be combined to accentuate features of interest, blending green (band 3), red (band 4) and infrared (band 8) creates a "false colour" image amplifying the infrared band. In 2020, Amnesty International used false colour imagery to identify vegetation burnt by gun propellant to pinpoint the firing position of Tigray strikes on Eritrea. [@https://citizenevidence.org/author/wallace-fan2023]Hundreds of [standard combinations](https://www.indexdatabase.de/db/i.php?offset=1) exist with the majority used to highlight vegetation, its water content and the chemical composition of the surrounding soil for use in crop monitoring.The images below show the bombing of Raqqa on October 10th, 2017 in different frequency bands. The real colour image clearly shows the plume of smoke over the city, but the SWIR band (based on bands 12, 8A and 4) shows individual fires. The Normalised Difference Moisture Index (NDMI) appears bright red over the smoke indicating the low level of humidity here, the bluer sections at the edges are fields so have a higher level of moisture.

```{r, echo=FALSE,out.width="33%", out.height="33%",fig.cap="Images from October 10 2017 in different frequency bands, showing real colour (left), SWIR (centre) and moisture index (right)",fig.show='hold',fig.align='left'}
knitr::include_graphics(c("../Visualisations/Raqqa Real Colour.png",
                          "../Visualisations/Raqqa SWIR.png",
                          
                          "../Visualisations/Raqqa Water Stress.png"))
```

To choose suitable bands for identifying building destruction, I followed a simplified version of the methodology described by Pfeiffle (2022) and Putri et al. (2022). [@sandhiniputri2022] [Conflict identification] Pfeiffle used machine learning methods to automatically identify building damage from airstrikes in Iraq and Northern Syria, choosing all Sentinel-2 images with a 10m resolution (red, green, blue and NIR). Assuming the damage profile of bombed buildings would be similar to those damaged in earthquakes, I also downloaded Sentinel-1 GRD imagery as Putri et al. concluded a fusion of Sentinel-1 and Sentinel-2 data yielded the highest accuracy in their random forest model identifying building damage from the 2018 Lombok earthquake. It would also be interesting to explore the possibility of using phase or polarisation parameters in the Sentinel-1 data to pinpoint destruction.

### Data Download Options

Sentinel Satellite data is available free online through the following portals:

-   [EO Browser](https://apps.sentinel-hub.com/eo-browser/?zoom=10&lat=41.9&lng=12.5&themeId=DEFAULT-THEME&toTime=2023-04-26T12%3A08%3A08.207Z)
-   [Copernicus Open Access Hub AP](https://scihub.copernicus.eu/)I
-   [Sentinel Hub API](https://docs.sentinel-hub.com/api/latest/api/process/)

The EO browser is an interface allowing registered users to easily select and download satellite imagery of regions of interest from an interactive map. Using simple drop down menus users can also select the relevant time range, level of maximum cloud coverage and type of image. Results are presented as a simple image list which not only offers the option of fusing single images into standard combinations including real colour, false colour and Normalised Difference Vegetation Index (NDVI), but also of creating custom combinations. Images can be saved, shared, pinned, downloaded and automatically woven into time lapse sequences.

Alternatively Sentinel-1, 2 and 3 imagery can be downloaded via the Copernicus Hub API. The R package [sen2r](https://cran.r-project.org/web/packages/sen2r/index.html) has been written to simplify the process of accessing Sentinel-2 data, allowing users to access imahergy not only via the libraries custom functions but also through a Shiny backed GUI interface. The RESTful Sentinel Hub Process API provides full access to data from all Sentinel satellites including polarisation and phase information. Users are offered the ability to both select products and perform bespoke image processing using the platform's Javascript based Evalscript. (Example of commented evalscript from code)

Although it's possible to easily visualise, download and customise satellite imagery using EO Browser and sen2r, this project downloaded all imagery directly from the Sentinel Hub API. To use satellite data in a potential machine learning project I wanted to create an efficient data pipeline to collect and process large numbers of satellite photos without scrolling through multiple images manually via the browser. The sen2R package does offer this functionality but only for Sentinel-2 data, also it requires sci hub log-in which is only activated one week after registration (and would have made meeting this deadline impossible :))

### Accessing the Sentinel Hub API

Configuring the OAUTH 2.0 authorisation in R was a challenging task, first it was necessary to register for a client ID and secret then create an oauth app to generate a token (which is valid for 1 hour) The Sentinal API requires a POST request specifying the endpoint and request body. The`{r, eval=FALSE} add_headers` argument must also be set, requiring: png images are returned `{r, eval=FALSE} Accept`;the body request will be sent in JSON `{r, eval=FALSE} Content-Type` and specifying the necessary authorisation parameters.

```{r, eval=FALSE}
library(httr)
library(brio)# for loading text file with no extension
library(rio)
library(png)

### --- Oauth Authorisation to access API

# IDs and secrets
client_id <- readRDS("Secrets/oauth_client_id.rds")
client_secret <- readRDS("Secrets/oauth_secret.rds")

# Create the app
app <- oauth_app(appname = "SatelliteImageryAnalysis",
                 key = client_id,
                 secret = client_secret)

# Create the oauth endpoint
endpoint <- oauth_endpoint(
  request = NULL,
  authorize = NULL,
  access = "token",
  base_url = "https://services.sentinel-hub.com/oauth")

# Cache the token to prevent API being called multiple times (token lasts 1 hour)
options(httr_oauth_cache=T) # prevents pop-up asking to cache token
token <- oauth2.0_token(endpoint = endpoint,
                        app = app,
                        use_basic_auth = T,
                        client_credentials = T)
```

Using a combination of [these parameters](https://docs.sentinel-hub.com/api/latest/reference/#tag/catalog_core/operation/getCatalogLandingPage) coupled with [these examples](https://docs.sentinel-hub.com/api/latest/api/process/examples/) it was possible to structure the following request in JSON format. The upper portion of the JSON specifies the date range, region of interest (bbox); acceptable level of cloud cover (maxCloudCoverage);type of satellite footage required (type), resolution (width, height) and coordinate system (crs) while the lower section consists of evalscript to process the imagery.

```{python, eval=FALSE}
  request={
    "input": {
        "bounds": {
            "properties": {
                "crs": "http://www.opengis.net/def/crs/OGC/1.3/CRS84"
            },
            "bbox": [
                13.822174072265625,
                45.85080395917834,
                14.55963134765625,
                46.29191774991382
            ]
        },
        "data": [
            {
                "type": "sentinel-2-l2a",
                "dataFilter": {
                    "timeRange": {
                        "from": "2018-10-01T00:00:00Z",
                        "to": "2018-12-31T00:00:00Z"
                    }
                }
            }
        ]
    },
    "output": {
        "width": 512,
        "height": 512
    }
}
```

Ideally the POST request should be sent as a multipart form, which would allow the evalscript to be written in ordinary JSON. HoweverI was unable to do this correctly, so as a workaround escaped the entire JSON Evalscript section (using [this tool)](https://www.freeformatter.com/json-escape.html) and sent it as part of the same request. The image information was then extracted from the response object and the file saved as png.

```{r,eval=FALSE}
### --- Build request

request <- read_file("R Scripts/POST body request") # loads request from JSON in text file
sentinel_endpoint <- "https://services.sentinel-hub.com/api/v1/process"

# Specify which formats are accepted in request(JSON) and response(png)
response <- httr::POST(sentinel_endpoint, 
                       body=request, 
                       add_headers(Accept = "image/png", 
                                   `Content-Type`="application/json", 
                                   Authorization = paste("Bearer", token$credentials[[1]], sep = " ")))
```

### Function to automatically download Sentinel-2 Data

To simplify the download process I converted this code to a function to download the Sentinel-2 data automatically. The function handles the authentication process and accepts the following arguments:

```{r, eval=FALSE}
download_satellite_data(start_date, period_length_days, number_periods, min_lat,max_lat,min_long,max_long,satellite_type,cloud_cover,pixels_dimensions_height)

# Minimum and maximum latitudes and longitudes specify region of interest
# period_length, start_date and number of periods can be combined to specify data collection period
# For example: download_satellite_data("2017-05-01", 7, 32)
# Would return the best cloud free image taken every week from May 2017- September 2017 (32 in total)
# pixels_dimensions_height specifies the horizontal dimension of the output image in pixels

```

To make the file structure as clear as possible the JSON body of the request was stored in the separate text file ("POST body request.txt") and imported to the R file containing the function ("Download Sentinel Satellite Data.R". Str_replace() was used to replace the existing parameters with the user defined function arguments in the JSON code. str().

```{r, eval=FALSE}
  request_with_start <- str_replace(request, "InsertStartDateHere", start_date_as_string)
  request_with_end <- str_replace(request_with_start, "InsertEndDateHere", end_date_as_string)
  
  ### Create satellite type parameter of request body----------
  request_with_satellite <- str_replace(request_with_end, "InsertSatelliteTypeHere", satellite_type)
  
  ### Create cloud cover parameter of request body----------
  request_with_cloudcover <- str_replace(request_with_satellite, "InsertCloudCoverHere", as.character(cloud_cover))
```

It is easiest to define the area of interest using the maximum and minimum longitudes and latitudes, however using the bbox argument in the JSON selected a much larger area than expected, so this function uses the type **Polygon** parameter instead. This argument takes a series of geo-coordinates and joins them up so the first and last coordinate must be the same to close the shape.

It was also necessary to specify the dimensions of the returned images as selecting width and height at random led to distorted, skewed pictures. Using the code below the size of region of interest in degrees was calculated then converted to kilometres using standard formulae. By calculating the ratio of the sides of the rectangle, the optimum height width ratio was calculated and combined with the user's choice of pixels_dimensions_height to define the optimal image dimensions.

```{r, eval=FALSE}
 ### Calculate optimum image size----
  # API demands user specifies the dimensions of the output image
  # To ensure it is not stretched, dimensions have been calculated based on user specifying desired number of pixels in horizontal direction
  
  # Calculate longitude and latitude differences from user input
  latitude_diff <- max_lat-min_lat
  longitude_diff <- max_long-min_long
  
  #Convert difference in latitude from degrees to km using standard conversion formula
  lat_metres <- abs(111.32*latitude_diff)
  long_metres <-  abs(longitude_diff*40075*cos(max_lat)/360)
  
  # Calculate ratio of height:length of the selected satellite image
  normalised_lat_km <- lat_metres/long_metres
  normalised_lat_km 
  
  # Calculate width of output image based on user defined pixel height
  pixels_dimensions_width= round(normalised_lat_km*pixels_dimensions_height)
```

Finally the date the image was taken was added to the image, this was simple as the date (current_date) was already saved in memory.

```{r, eval=FALSE}
 ### Annotate png with Date---
  
#Extract date parameters to display
  year <- lubridate::year(current_date_start)
  month <- lubridate::month(current_date_start, label=TRUE)
  day <- lubridate::day(current_date_start)
  
  complete_date <- paste(day, month, year, sep=" ")
  
  processed_png <- image_read(data_filename)
  annotated_png <- image_annotate(processed_png, complete_date,location = "+10+450", size = 30, color = "white")
```

### Data Visualisation: Coalition Airstrikes Against Raqqa: May-October 2017

Using [this tutorial](file:///C:/Users/amand/Zotero/storage/WBTPXVZT/how-to-build-a-gif-of-satellite-imagery-in-r.html) an animated GIF was created from the downloaded satellite imagery showing a stop animation of the 4-month bombardment of Raqqa.The animation clearly shows the destruction of the city starting in the suburbs in the North West of the city and gradually moving towards the centre. On October 6th a huge plume of black smoke is clearly visible.

<img src="../Visualisations/raqqa2.gif" align="left"/>

```{r gapgif, eval=FALSE}
library(dplyr)
library(purrr) 
library(magick) #For animation

raqqa_gif2 <- list.files(path = "../Sentinel 2A Imagery/", pattern = "*.png", full.names = T) %>% 
  map(image_read) %>% 
  image_join() %>% 
  image_annotate("Airstrikes on Raqqa, May 2017: November 2017", location = "+10+10", size = 20, color = "white")%>%
  image_animate(fps=1) %>% #controls speed of animation
  image_write("raqqa2.gif") 

```

\

\

```{r cars}
summary(cars)
```
