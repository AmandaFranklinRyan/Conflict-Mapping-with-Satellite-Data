---
title: "Propaganda Wars"
author: "Amanda Franklin-Ryan"
date: "2023-04-25"
output:
  html_document:
    code_folding: hide
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

### Atrocity Propaganda

*"History stopped in 1936- after that, there was only propaganda"* - **George Orwell**

*"We are children no more"*, 15-year-old 'Nayirah'" tells the Congressional Caucus, blinking back tears, her voice cracking as she describes Iraqi soldiers storming the hospital where she volunteered, stealing incubators and throwing babies onto the floor to die.[@guyjohn592010] It's 1991, Vietnam's legacy still looms large and the American public have no desire to be embroiled in yet another distant war, but Nayirah's testimony sways them with the US invading Iraq 3-months later. Except it wasn't true, any of it. Nayirah al-Sabah, the daughter of the Kuwaiti ambassador had never set foot in the hospital, instead she was coached to perfection for her Oscar winning performance by PR firm Hill and Knowlton who were directly employed by the Kuwaiti Government.[@abuhamad1994] [@howfals]

Atrocity propaganda is nothing new, with lurid tales of fabricated massacres stretching back to the Crusades, but now storytelling is easier than ever before; all you need is a mobile and internet connection. There are now more hours of Syrian conflict footage online, than time elapsed since the war started.Almost five million people are currently trapped in Khartoum as Sudanese generals Abdel Fattah al-Burhan and Mohamed Hamdan Daglo vie for power and Twitter is awash with fake footage taken from conflicts in Yemen, Libya and even a computer game.[@tanksan] With most Twitter accounts geolocation data switched off, it's game on for opportunists worldwide to create and share their own realities. Even with image metadata and reverse image search, it's almost impossible to ascertain whether shared images are accurate, misrepresentative, staged or concocted by government backed bots. [DALL.E2](https://openai.com/product/dall-e-2) will only make things worse, with the potential power to conjure up bespoke atrocities in every style from gritty World War II grayscale to heartrending full colour closeup at the click of a mouse.

Yet in a world where fiction seems higher definition than reality, satellite imagery could offer a glimmer of hope. In March 2022 Ukrainians reported the rape, murder and torture of hundreds of civilians in Bucha, with bodies strewn across the streets. Unsurprisingly, the Russian Defence Ministry immediately dismissed the allegations as, "another production of the Kyiv regime for the Western media", blaming Ukrainian soldiers themselves for the killings. [@mirovalev]However Maxar satellite data corroborated the Ukrainian narrative illustrating the bodies had been present long before Ukrainian forces entered the area.[@buchaki2022]

While high resolution Maxar data is only available commercially or for selected humanitarian disasters, medium resolution Sentinel-1 and Sentinel 2 satellite imagery is freely accessible online. Evidently it's no magic bullet. evidence of newly dug mass graves or bombed buildings, proves only the incident is not a recycled copy-paste of a past atrocity and in isolation can rarely identify the true perpetrator, however it is already being used in conflict mapping. Depending on the location and nature of the violence, burning oil wells; abandoning agricultural land, (pictures here) construction of refugee camps and building damage have been used to identify and monitor armed conflict. Using methods to map meteor craters on the moon, a 2020 study used satellite imagery combined with machine learning to identify unexploded munitions from the Vietnam War embedded in overgrown bomb craters in rural Cambodia.[@lin2020] Detecting chemical and biological warfare with satellite imagery is less promising due to the low concentrations of the agents in the atmosphere, however vegetation changes could potentially be use as a proxy for contamination with radioactivity [@sutlieff2021] and journalists are already using satellite pictures to investigate North Korea's chemical weapon capabilities.[@abirds]

### Raqqa: The Perfect Perpetratorless Crime

Launched in 2019, Amnesty International's [Strike Tracker](https://decoders.amnesty.org/projects/strike-tracker#decode-results) used satellite imagery coupled with crowd sourced image labelling to create an accurate timeline for the 2017 US led Coalition bombing in the Syrian city of Raqqa. Attempting to destroy Islamic State's "Caliphate", Coalition forces fired over 30,000 missiles in a four-month bombardment, destroying 80% of the city and killing and injuring thousands of civilians. Pinning down responsibility was challenging as the Coalition accused ISIS, Russia blamed the US and the US denied all responsibility ruling less than one percent of civilian casualty reports "credible".[@april252019]

Amnesty International enlisted thousands of digital activists to analyse a series of satellite photos of the same building to identify the exact date it was destroyed. After following a short interactive tutorial volunteers were presented with a new series of images to label. Combining the crowd sourced strike timeline with witness testimony and interviews, Amnesty forced the Coalition to admit responsibility for 159 civilian deaths.[@isconfl2019]

This project aims to collect the dataset necessary to replicate Amnesty's Strike Tracker Project. However instead of using high resolution human labelled satellite images, medium resolution Sentinel-1 and Sentinel-2 images will be downloaded and in a future seminar paper (hopefully) used to investigate the efficacy of using machine learning methods to automatically detect building damage and to generate a destruction timeline using a simplified version of the methodology described in x. More specifically the project will:

-   Identify and download suitable satellite images from the SentinelHub API, an R wrapper will also be written to simplify this process

-    Collect and visualise data from UNOSAT illustrating the position of the coalition strikes (for the purposes of the future machine learning model this represents the ground truth)

-    Download series of news articles from Guardian/New York Times API illustrating who different media outlets hold responsible for the civilian deaths.

## Medium Resolution Satellite Data (Sentinel-1 and Sentinel-2)

### Overview

Launched by the European Space Agency in June 2015, primarily for agricultural monitoring and emergency management, Sentinel-2 collects land and ocean imagery to a maximum resolution of 10m every 5 days. [@sentinel]The imagery consists of 13 spectral bands ranging from visible (red, green, blue) to near infrared (NIR) and short wave infrared (SWIR), it also includes specific frequencies for identifying vegetation (bands 5-8a) and clouds (Band 10: Cirrus).[@customs]

Sentinel -1 was launched by the European Commission's Copernicus Project a year earlier in April 2014, and collects Synthetic Aperture Radar (SAR) data every 10 days. [@sentinela]While Sentinel 2's optical sensors allow it to capture imagery from reflected sunlight, Sentinel-1 produces its own electromagnetic beam (with 8 bands between 0.3GHz and 40GHz) and records its reflection. The longer wavelength radar frequencies not only pass straight through clouds so can be used in any weather, but penetrate soil and sand [revealing hidden underground artefacts](https://earthobservatory.nasa.gov/features/SpaceArchaeology). Additionally, phase data can be extracted from the radar signal and used to calculate differences in surface topography with an accuracy of up to a few centimetres. [@earthsciencedatasystems2020] Combining these height difference calculations with machine learning techniques, one 2022 study identified buildings damaged by earthquakes.[@earthsciencedatasystems2020]

Real colour images can be constructed by superimposing red, green and blue bands but often it is more revealing to use false colour. The optimal frequency depends on the exact nature of the task. Infrared is generally preferable for forestry mapping as soil and vegetation are difficult to differentiate in true colour imagery (as their spectral reflectances are almost identical in the visible range), however there is a difference of almost 30% in the mid-infrared band. [@customs]

Frequencies can also be combined to accentuate features of interest, blending green (band 3), red (band 4) and infrared (band 8) creates a "false colour" image amplifying the infrared band. In 2020, Amnesty International used false colour imagery to identify vegetation burnt by gun propellant to pinpoint the firing position of Tigray strikes on Eritrea. [@https://citizenevidence.org/author/wallace-fan2023]Hundreds of [standard combinations](https://www.indexdatabase.de/db/i.php?offset=1) exist with the majority used to highlight vegetation, its water content and the chemical composition of the surrounding soil for use in crop monitoring.(include experimental imagery here from EO browser, Raqqa, destroyed buildings in blue+ resolution etc, talk about trade off with resolution, size of antenna, include formulae for some of the different combinations)

```{r, echo=FALSE,out.width="49%", out.height="20%",fig.cap="caption",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("..Visualisations//EO Browser Real Colour Rainforest Fires.png",
                          "..Visualisations//EO Browser False Colour Rainforest Fires.png"))
```

To choose suitable bands for identifying building destruction, I followed a simplified version of the methodology described by Pfeiffle (2022) and Putri et al. (2022). [@sandhiniputri2022] [Conflict identification] Pfeiffle used machine learning methods to automatically identify building damage from airstrikes in Iraq and Northern Syria, choosing all Sentinel-2 images with a 10m resolution (red, green, blue and NIR). Assuming the damage profile of bombed buildings would be similar to those damaged in earthquakes, I also downloaded Sentinel-1 GRD imagery as Putri et al. concluded a fusion of Sentinel-1 and Sentinel-2 data yielded the highest accuracy in their random forest model identifying building damage from the 2018 Lombok earthquake. It would also be interesting to explore the possibility of using phase or polarisation parameters in the Sentinel-1 data to pinpoint destruction.

### Data Download Options

Sentinel Satellite data is available free online through the following portals:

-   [EO Browser](https://apps.sentinel-hub.com/eo-browser/?zoom=10&lat=41.9&lng=12.5&themeId=DEFAULT-THEME&toTime=2023-04-26T12%3A08%3A08.207Z)
-   [Copernicus Open Access Hub AP](https://scihub.copernicus.eu/)I
-   [Sentinel Hub API](https://docs.sentinel-hub.com/api/latest/api/process/)

The EO browser is an interface allowing registered users to easily select and download satellite imagery of regions of interest from an interactive map. Using simple drop down menus users can also select the relevant time range, level of maximum cloud coverage and type of image. Results are presented as a simple image list which not only offers the option of fusing single images into standard combinations including real colour, false colour and Normalised Difference Vegetation Index (NDVI), but also of creating custom combinations. Images can be saved, shared, pinned, downloaded and automatically woven into time lapse sequences. (example video of Raqqa.)

Alternatively Sentinel-1, 2 and 3 imagery can be downloaded via the Copernicus Hub API. The R package [sen2r](https://cran.r-project.org/web/packages/sen2r/index.html) has been written to simplify the process of accessing Sentinel-2 data, allowing users to access imahergy not only via the libraries custom functions but also through a Shiny backed GUI interface. The RESTful Sentinel Hub Process API provides full access to data from all Sentinel satellites including polarisation and phase information. Users are offered the ability to both select products and perform bespoke image processing using the platform's Javascript based Evalscript. (Example of commented evalscript from code)

Although it's possible to easily visualise, download and customise satellite imagery using EO Browser and sen2r, this project downloaded all imagery directly from the Sentinel Hub API. To use satellite data in a potential machine learning project I wanted to create an efficient data pipeline to collect and process large numbers of satellite photos without scrolling through multiple images manually via the browser. The sen2R package does offer this functionality but only for Sentinel-2 data, also it requires sci hub log-in which is only activated one week after registration (and would have made meeting this deadline impossible :))

### Accessing the Sentinel Hub API

Configuring the OAUTH 2.0 authorisation in R was a challenging task, first it was necessary to register for a client ID and secret then create an oauth app to generate a token (which is valid for 1 hour) The Sentinal API requires a POST request specifying the endpoint and request body. The`{r, eval=FALSE} add_headers` argument must also be set, requiring: png images are returned `{r, eval=FALSE} Accept`;the body request will be sent in JSON `{r, eval=FALSE} Content-Type` and specifying the necessary authorisation parameters.

```{r, eval=FALSE}
library(httr)
library(brio)# for loading text file with no extension
library(rio)
library(png)

### --- Oauth Authorisation to access API

# IDs and secrets
client_id <- readRDS("Secrets/oauth_client_id.rds")
client_secret <- readRDS("Secrets/oauth_secret.rds")

# Create the app
app <- oauth_app(appname = "SatelliteImageryAnalysis",
                 key = client_id,
                 secret = client_secret)

# Create the oauth endpoint
endpoint <- oauth_endpoint(
  request = NULL,
  authorize = NULL,
  access = "token",
  base_url = "https://services.sentinel-hub.com/oauth")

# Cache the token to prevent API being called multiple times (token lasts 1 hour)
options(httr_oauth_cache=T) # prevents pop-up asking to cache token
token <- oauth2.0_token(endpoint = endpoint,
                        app = app,
                        use_basic_auth = T,
                        client_credentials = T)
```

Using a combination of [these parameters](https://docs.sentinel-hub.com/api/latest/reference/#tag/catalog_core/operation/getCatalogLandingPage) coupled with [these examples](https://docs.sentinel-hub.com/api/latest/api/process/examples/) it was possible to structure the following request in JSON format. The upper portion of the JSON specifies the date range, bounding box, acceptable level of cloud cover and type of satellite footage required, while the lower section consists of evalscript to process the imagery.

```{=html}
<pre style="background-color:black;><span style="color: white;">{
    "input": {
        "bounds": {
            "properties": {
                <abbr title="Specify coordinate system">"crs"</abbr>: "http://www.opengis.net/def/crs/OGC/1.3/CRS84"
            },
            <abbr title="Bounding box: Coordinates illustrating region of interest">"bbox"</abbr>: [
                38.93641,
                39.05947,
                35.92829,
                35.97450
            ]
        },
        "data": [
            {
                "type": "sentinel-2-l1c",
                "dataFilter": {
                    "timeRange": {
                        "from": "2019-10-01T00:00:00Z",
                        "to": "2019-12-31T00:00:00Z"
                    },
                    <abbr title="Maximum level of acceptable cloud coverage (0-100%)">"maxCloudCoverage"</abbr>:5
                }
            }
        ]
    },
    <abbr title="Specify size of downloaded images">"output":</abbr> {
        "width": 2000,
        "height": 2000
    }
},
    <span style="color: white;">"evalscript"</span>: "//VERSION=3\n\nfunction setup() {\n  return {\n    input: [\"B02\", \"B03\", \"B04\"],\n    output: { \n      bands: 3,\n      sampleType: \"AUTO\" // default value - scales the output values from [0,1] to [0,255].   \n     }\n  }\n}\n\nfunction evaluatePixel(sample) {\n  return [2.5 * sample.B04, 2.5 * sample.B03, 2.5 * sample.B02]\n}"
}
</pre>
```
text</span></strong>

Ideally the POST request should be sent as a multipart form, which would allow the evalscript to be written in ordinary JSON. HoweverI was unable to do this correctly, so as a workaround escaped the entire JSON Evalscript section (using [this tool)](https://www.freeformatter.com/json-escape.html) and sent it as part of the same request. The image information was then extracted from the response object and the file saved as png.

```{r,class.source = 'fold-show', eval=FALSE}
### --- Build request

request <- read_file("R Scripts/POST body request") # loads request from JSON in text file
sentinel_endpoint <- "https://services.sentinel-hub.com/api/v1/process"

# Specify which formats are accepted in request(JSON) and response(png)
response <- httr::POST(sentinel_endpoint, 
                       body=request, 
                       add_headers(Accept = "image/png", 
                                   `Content-Type`="application/json", 
                                   Authorization = paste("Bearer", token$credentials[[1]], sep = " ")))
```

\

\

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
